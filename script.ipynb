{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "048d1d0f-8c94-4f48-9ed8-66f9b289d55f",
    "_uuid": "8ba2b81da7112bb2c444597f0c683da06f46b532"
   },
   "source": [
    "This is fast image downloader using this trick:\n",
    "https://www.kaggle.com/c/landmark-recognition-challenge/discussion/49703\n",
    "And you can change target size that you prefer.\n",
    "\n",
    "Reference:\n",
    "https://www.kaggle.com/c/landmark-recognition-challenge/discussion/48895\n",
    "```\n",
    "For 256,256 this should be 22 GB\n",
    "For 224,224 this should be 16.8 GB\n",
    "For 139,139 this should be 6.5 GB\n",
    "For 128,128 this should be 5.5 GB\n",
    "For 96,96 this should be 3.1 GB\n",
    "For 64,64 this should be 1.4 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e097aec8-4b08-4a52-a2ea-356ba28f23ca",
    "_uuid": "37602ab4582f8bb69e165a7c52f33de7a020d480",
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "from io import BytesIO\n",
    "#from urllib import request\n",
    "import urllib2\n",
    "import pandas as pd\n",
    "import re\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# set files and dir\n",
    "DATA_FRAME, OUT_DIR = pd.read_csv('train.csv'), 'train'  # recognition challenge\n",
    "# DATA_FRAME, OUT_DIR = pd.read_csv('../input/index.csv'), '../input/index'  # retrieval challenge\n",
    "# DATA_FRAME, OUT_DIR = pd.read_csv('../input/test.csv'), '../input/test'  # test data\n",
    "\n",
    "# preferences\n",
    "TARGET_SIZE = 128  # image resolution to be stored\n",
    "IMG_QUALITY = 90  # JPG quality\n",
    "NUM_WORKERS = 8  # Num of CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "e09be776-5baa-41c8-a99b-a56c49124d3c",
    "_uuid": "d6e1daa19a1099b1cff18c466598cabc5527ab06",
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s1600       987672\n",
       "rj           94246\n",
       "medium       92054\n",
       "original     11529\n",
       "s128          9571\n",
       "Name: url, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_FRAME.url.apply(lambda x: x.split('/')[-2]).value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a4ad759b-4014-4477-b75c-af10e4f718d1",
    "_uuid": "71befb4791887baf9a9f18c3d365aaae94c88066"
   },
   "source": [
    "We found that almost images have 1600x resolution.\n",
    "Downloading such a high resolution images takes so much time, so I recommend you to download images after changing url \"s1600\" to \"s{TARGET_SIZE}\" like the below script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b713704e-8e75-4b12-9620-d0b582ad7f7d",
    "_uuid": "af08d9b8e92a4a532b1425814bbd5593638763d3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overwrite_urls(df):\n",
    "    def reso_overwrite(url_tail, reso=TARGET_SIZE):\n",
    "        pattern = 's[0-9]+'\n",
    "        search_result = re.match(pattern, url_tail)\n",
    "        if search_result is None:\n",
    "            return url_tail\n",
    "        else:\n",
    "            return 's{}'.format(reso)\n",
    "\n",
    "    def join_url(parsed_url, s_reso):\n",
    "        parsed_url[-2] = s_reso\n",
    "        return '/'.join(parsed_url)\n",
    "\n",
    "    parsed_url = df.url.apply(lambda x: x.split('/'))\n",
    "    train_url_tail = parsed_url.apply(lambda x: x[-2])\n",
    "    resos = train_url_tail.apply(lambda x: reso_overwrite(x, reso=TARGET_SIZE))\n",
    "\n",
    "    overwritten_df = pd.concat([parsed_url, resos], axis=1)\n",
    "    overwritten_df.columns = ['url', 's_reso']\n",
    "    df['url'] = overwritten_df.apply(lambda x: join_url(x['url'], x['s_reso']), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_data(df):\n",
    "    key_url_list = [line[:2] for line in df.values]\n",
    "    return key_url_list\n",
    "\n",
    "\n",
    "def download_image(key_url):\n",
    "    (key, url) = key_url\n",
    "    filename = os.path.join(OUT_DIR, '{}.jpg'.format(key))\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        print('Image {} already exists. Skipping download.'.format(filename))\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        response = request.urlopen(url)\n",
    "        image_data = response.read()\n",
    "    except:\n",
    "        print('Warning: Could not download image {} from {}'.format(key, url))\n",
    "        return 1\n",
    "\n",
    "    try:\n",
    "        pil_image = Image.open(BytesIO(image_data))\n",
    "    except:\n",
    "        print('Warning: Failed to parse image {}'.format(key))\n",
    "        return 1\n",
    "\n",
    "    try:\n",
    "        pil_image_rgb = pil_image.convert('RGB')\n",
    "    except:\n",
    "        print('Warning: Failed to convert image {} to RGB'.format(key))\n",
    "        return 1\n",
    "\n",
    "    try:\n",
    "        pil_image_resize = pil_image_rgb.resize((TARGET_SIZE, TARGET_SIZE))\n",
    "    except:\n",
    "        print('Warning: Failed to resize image {}'.format(key))\n",
    "        return 1\n",
    "\n",
    "    try:\n",
    "        pil_image_resize.save(filename, format='JPEG', quality=IMG_QUALITY)\n",
    "    except:\n",
    "        print('Warning: Failed to save image {}'.format(filename))\n",
    "        return 1\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def loader(df):\n",
    "    if not os.path.exists(OUT_DIR):\n",
    "        os.mkdir(OUT_DIR)\n",
    "\n",
    "    key_url_list = parse_data(df)\n",
    "    pool = multiprocessing.Pool(processes=50)\n",
    "    #pool.map(download_image, key_url_list)\n",
    "    failures = sum(tqdm.tqdm(pool.imap_unordered(download_image, key_url_list),\n",
    "                            total=len(key_url_list)))\n",
    "    print('Total number of download failures:', failures)\n",
    "    pool.close()\n",
    "    pool.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5a8fb78f-c46e-4b85-95e2-40736aa1e9ec",
    "_uuid": "6a17d89433c4bdb281ba7a1dd564b260181c7fd9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, start downloading\n",
    "if __name__ == '__main__':\n",
    "    loader(overwrite_urls(DATA_FRAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
