{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snappy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3cd762eecff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msnappy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'snappy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import snappy\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import cPickle\n",
    "import os\n",
    "import numpy.random as rd\n",
    "from math import sqrt\n",
    "from sklearn.cluster import KMeans\n",
    "import multiprocessing as mtp\n",
    "import time\n",
    "\n",
    "root_dir = os.environ['CAFFE_PROJ_DIR']\n",
    "sys.path.append(os.path.join(root_dir, 'python/caffe/proto/'))\n",
    "import caffe_pb2\n",
    "\n",
    "class UnpickleError(Exception):\n",
    "    pass\n",
    "\n",
    "def test_label(prob_dir, index, gt_label):\n",
    "    datum = caffe_pb2.Datum()\n",
    "    \n",
    "    fn = '%010d' % index\n",
    "    f = open(os.path.join(prob_dir, fn), 'rb')\n",
    "    data = f.read()\n",
    "    f.close()\n",
    "    datum.ParseFromString(snappy.decompress(data))\n",
    "    pred_lb = np.argmax(np.asarray(datum.float_data))\n",
    "    return 1 if pred_lb == gt_label else 0\n",
    "\n",
    "\n",
    "def read_layer_ftr(ftr_dir, index):\n",
    "    fn = os.path.join(ftr_dir, '%010d' % index)\n",
    "    try:\n",
    "        f = open(fn, 'rb')\n",
    "    except IOError:\n",
    "        print('can not open file %s' % fn)\n",
    "        return None\n",
    "    datum = caffe_pb2.Datum()\n",
    "    datum.ParseFromString(snappy.uncompress(f.read()))\n",
    "    f.close()\n",
    "    return np.asarray(datum.float_data, dtype=np.single)\n",
    "\n",
    "def read_train_image_list(tr_img_list_file):\n",
    "    # read a list of ordered training images\n",
    "    f = open(tr_img_list_file, 'r')\n",
    "    tr_img_names, tr_img_labels, tr_class_start = [], [], []\n",
    "    cur_lb = -1\n",
    "    for line in f:\n",
    "        line = line.split(' ')\n",
    "        lb = int(line[1])\n",
    "        if not  lb == cur_lb:\n",
    "            tr_class_start += [len(tr_img_names)]\n",
    "            cur_lb = lb\n",
    "        tr_img_names += [line[0]]\n",
    "        tr_img_labels += [lb]   \n",
    "    f.close()\n",
    "     \n",
    "    print ('% d training images ' % len(tr_img_names))\n",
    "    num_tr_imgs = len(tr_img_names)\n",
    "    num_classes = len(tr_class_start)\n",
    "    tr_class_end = tr_class_start[1:] + [num_tr_imgs]\n",
    "    tr_class_start, tr_class_end = np.asarray(tr_class_start), np.asarray(tr_class_end)\n",
    "    tr_class_size = tr_class_end - tr_class_start\n",
    "    assert sum(tr_class_size) == num_tr_imgs\n",
    "    print ('%d classes' % num_classes)\n",
    "#     plt.hist(tr_class_size,bins=20)\n",
    "#     plt.title('class size histogram')\n",
    "    return tr_img_names, tr_img_labels, tr_class_start, tr_class_end\n",
    "\n",
    "def write_train_image_list(tr_img_list_file, tr_img_names, tr_img_labels):\n",
    "    assert(len(tr_img_names) == len(tr_img_labels))\n",
    "    f = open(tr_img_list_file, 'w')\n",
    "    for i in range(len(tr_img_names)):\n",
    "        f.write('%s %d' % (tr_img_names[i], tr_img_labels[i]))\n",
    "    f.close()\n",
    "\n",
    "def read_val_image_list(val_img_list_file):    \n",
    "    # read a list of validation images\n",
    "    f = open(val_img_list_file, 'r')\n",
    "    val_img_names, val_img_labels = [], []\n",
    "    for line in f:\n",
    "        line = line.split(' ')\n",
    "        lb = int(line[1])\n",
    "        val_img_names += [line[0]]\n",
    "        val_img_labels += [lb]    \n",
    "    f.close()\n",
    "    print ('%d validation images ' % len(val_img_names))\n",
    "    return val_img_names, val_img_labels\n",
    "\n",
    "def softmax(ftr):\n",
    "    print ('compute softmax probabilities')\n",
    "    num, dim = ftr.shape[0], ftr.shape[1]\n",
    "    print ('num %d dim %d' % (num, dim))\n",
    "    prob = np.zeros((num, dim), dtype=np.single)\n",
    "    for i in range(num):\n",
    "        max_val = np.max(ftr[i, :])\n",
    "        row = ftr[i, :] - max_val\n",
    "        exp_val = np.exp(row)\n",
    "        prob[i, :] = exp_val / np.sum(exp_val)\n",
    "    return prob    \n",
    "    \n",
    "def spatial_softmax(ftr):\n",
    "    dim,h,w=ftr.shape[0],ftr.shape[1],ftr.shape[2]\n",
    "    spatial_prob=np.zeros((dim,h*w),dtype=np.single)\n",
    "    ftr=ftr.reshape((dim,h*w))\n",
    "    max_vals=np.max(ftr,axis=0)\n",
    "    ftr=ftr-max_vals[np.newaxis,:]\n",
    "    for i in range(h*w):\n",
    "        col = ftr[:,i] - max_vals[i]\n",
    "        exp_val = np.exp(col)\n",
    "        spatial_prob[:,i]=exp_val / np.sum(exp_val)\n",
    "    spatial_prob=spatial_prob.reshape((dim,h,w))\n",
    "    return spatial_prob\n",
    "        \n",
    "    \n",
    "''' compute a  histogram of the percentage of images \n",
    "that are classified as a label in the same cluster of its groundtruth label\n",
    "'''\n",
    "def plot_cluster_coverage(num_class, cluster_mbs, confusion_mat):\n",
    "    cluster_coverage = np.zeros((num_class))\n",
    "    for i in range(len(cluster_mbs)):\n",
    "        for j in range(len(cluster_mbs[i])):\n",
    "            class_id = cluster_mbs[i][j]\n",
    "            cluster_coverage[class_id] = np.sum(confusion_mat[class_id, cluster_mbs[i]])\n",
    "    n, bins, patches = plt.hist(cluster_coverage, bins=20)  \n",
    "    \n",
    "\n",
    "def read_text(fn):\n",
    "    text = []\n",
    "    f = open(fn)\n",
    "    for line in f:\n",
    "        text += [line]\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def write_text(fn, text):\n",
    "    f = open(fn, 'w')\n",
    "    f.writelines(text)\n",
    "    f.close()\n",
    "    \n",
    "def top_k_accuracy(pred_labels, gt_labels, top_k):\n",
    "    accu = {}\n",
    "    guesses = {}\n",
    "    for k in top_k:\n",
    "        assert pred_labels.shape[1] >= k\n",
    "        count = 0\n",
    "        num = len(gt_labels)\n",
    "        guess = [0] * num\n",
    "        for i in range(num):\n",
    "            for j in range(k):\n",
    "                if pred_labels[i, j] == gt_labels[i]:\n",
    "                    count += 1\n",
    "                    guess[i] = 1\n",
    "                    break\n",
    "        accu[str(k)] = float(count) / float(num)\n",
    "        guesses[str(k)] = guess\n",
    "    return accu, guesses\n",
    "\n",
    "def pickle(filename, data, compress=False):\n",
    "    if compress:\n",
    "        fo = zipfile.ZipFile(filename, 'w', zipfile.ZIP_DEFLATED, allowZip64=True)\n",
    "        fo.writestr('data', cPickle.dumps(data, -1))\n",
    "    else:\n",
    "        fo = open(filename, \"wb\")\n",
    "        cPickle.dump(data, fo, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    fo.close()\n",
    "    \n",
    "def unpickle(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        raise UnpickleError(\"Path '%s' does not exist.\" % filename)\n",
    "\n",
    "    fo = open(filename, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def find_layer_id(lay_names, lay_name):\n",
    "    for i in range(len(lay_names)):\n",
    "        if lay_names[i] == lay_name:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def compute_compression_factor(m, n, n_kmean_cluster, n_seg):\n",
    "    compress_factor = float(32 * m * n) / float(32 * n_kmean_cluster * n + m * n_seg * 16)\n",
    "    print ('compress_factor %4.3f' % compress_factor    )\n",
    "    \n",
    "def matrix_quantization(mat, n_kmean_cluster, n_seg):\n",
    "    st_time = time.time()\n",
    "    seg_size = mat.shape[1] / n_seg\n",
    "    assert (mat.shape[1] % n_seg) == 0\n",
    "    mat_indices = np.zeros((mat.shape[0], n_seg), dtype=np.int32)\n",
    "    mat_cluster_centers = np.zeros((n_kmean_cluster, mat.shape[1]), dtype=np.float32)\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=n_kmean_cluster, n_init=2, n_jobs=1, copy_x=False)\n",
    "    \n",
    "    display_num = 16\n",
    "    for i in range(n_seg):\n",
    "        start, end = i * seg_size, (i + 1) * seg_size\n",
    "        mat_seg = mat[:, start:end]\n",
    "        mat_indices[:, i] = kmeans.fit_predict(mat_seg)\n",
    "        if (i % (n_seg / display_num)) == 0:\n",
    "            print( mat_indices[:, i].shape, kmeans.cluster_centers_.shape)\n",
    "        mat_cluster_centers[:, start:end] = np.float32(kmeans.cluster_centers_)\n",
    "    \n",
    "    ep_time = time.time() - st_time\n",
    "    print ('elapsed time %5.2f' % ep_time)\n",
    "        \n",
    "    return mat_indices, mat_cluster_centers\n",
    "\n",
    "\n",
    "def fine_layer_id(net_param, layer_name):\n",
    "    if len(net_param.layer) > 0:\n",
    "        for i in range(len(net_param.layer)):\n",
    "            if layer_name == net_param.layer[i].name:\n",
    "                return i\n",
    "    else:\n",
    "        for i in range(len(net_param.layers)):\n",
    "            if layer_name == net_param.layers[i].name:\n",
    "                return i        \n",
    "    print ('fail to find layer %s' % layer_name)\n",
    "    return -1\n",
    "\n",
    "\n",
    "def quantiaztion_layer_parameter(args):\n",
    "    st_time = time.time()\n",
    "    \n",
    "    net_param_fn, layer_name, layer_name_prefix, n_kmean_cluster, n_seg, save_dir = \\\n",
    "    args[0], args[1], args[2], args[3], args[4], args[5]\n",
    "    print ('open net %s' % net_param_fn)\n",
    "    f = open(net_param_fn, 'rb')\n",
    "    net_param = caffe_pb2.NetParameter()\n",
    "    net_param.ParseFromString(f.read())\n",
    "    f.close()\n",
    "\n",
    "    layer_id = fine_layer_id(net_param, layer_name_prefix + layer_name)\n",
    "    \n",
    "    if len(net_param.layer) > 0:\n",
    "        parameter_layer = net_param.layer[layer_id]\n",
    "    else:\n",
    "        parameter_layer = net_param.layers[layer_id]\n",
    "    \n",
    "    print( parameter_layer.blobs[0].num, parameter_layer.blobs[0].channels, \\\n",
    "    parameter_layer.blobs[0].height, parameter_layer.blobs[0].width  )    \n",
    "    params = np.asarray(parameter_layer.blobs[0].data)\n",
    "    if parameter_layer.blobs[0].num > 1 and parameter_layer.blobs[0].channels > 1:\n",
    "        '''convolutional layer '''\n",
    "        params = params.reshape((parameter_layer.blobs[0].num, \n",
    "                                 parameter_layer.blobs[0].channels * \\\n",
    "                                 parameter_layer.blobs[0].height * \\\n",
    "                                 parameter_layer.blobs[0].width))\n",
    "    else:\n",
    "        '''fully-connected layer or CCCP layer'''\n",
    "        params = params.reshape((parameter_layer.blobs[0].height, parameter_layer.blobs[0].width))\n",
    "    \n",
    "    num_nan = np.sum(np.isnan(params))\n",
    "    if num_nan > 0:\n",
    "        print ('net_param_fn %s layer_name %s contains NaN number ' % (net_param_fn, layer_name) )\n",
    "    assert num_nan == 0\n",
    "    \n",
    "    compute_compression_factor(params.shape[0], params.shape[1], n_kmean_cluster[layer_name], n_seg[layer_name])\n",
    "\n",
    "    fc_indices, fc_cluster_centers = matrix_quantization(params, n_kmean_cluster[layer_name], n_seg[layer_name])\n",
    "\n",
    "    blob_proto = caffe_pb2.BlobProto()\n",
    "    blob_proto.num = 1\n",
    "    blob_proto.channels = 1\n",
    "    blob_proto.height = fc_cluster_centers.shape[0]\n",
    "    blob_proto.width = fc_cluster_centers.shape[1]\n",
    "    for j in range(fc_cluster_centers.shape[0]):\n",
    "        for k in range(fc_cluster_centers.shape[1]):\n",
    "            blob_proto.data.append(float(fc_cluster_centers[j, k]))\n",
    "    fn = save_dir + layer_name_prefix + layer_name + '_%d_%d' % (n_kmean_cluster[layer_name], n_seg[layer_name]) + \\\n",
    "    '_kmean_cluster_centers.binaryproto'\n",
    "    print ('write to %s ' % fn)\n",
    "    f = open(fn, \"wb\")\n",
    "    f.write(blob_proto.SerializeToString())\n",
    "    f.close()\n",
    "\n",
    "    blob_proto = caffe_pb2.BlobProto()\n",
    "    blob_proto.num = 1\n",
    "    blob_proto.channels = 1\n",
    "    blob_proto.height = fc_indices.shape[0]\n",
    "    blob_proto.width = fc_indices.shape[1]\n",
    "    for j in range(fc_indices.shape[0]):\n",
    "        for k in range(fc_indices.shape[1]):\n",
    "            blob_proto.data.append(float(fc_indices[j, k]))\n",
    "    fn = save_dir + layer_name_prefix + layer_name + '_%d_%d' % (n_kmean_cluster[layer_name], n_seg[layer_name]) + \\\n",
    "    '_kmean_cluster_indices.binaryproto'\n",
    "    print ('write to %s' % fn)\n",
    "    f = open(fn, \"wb\")\n",
    "    f.write(blob_proto.SerializeToString())\n",
    "    f.close()\n",
    "    \n",
    "    ep_time = time.time() - st_time\n",
    "    print( 'elapsed time %5.2f' % ep_time)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
